"""
MCPå›¾åƒè¯†åˆ«ç³»ç»Ÿ - æœ¬åœ°AIæ¨¡åž‹æä¾›è€…æ¨¡å—

è¿™ä¸ªæ¨¡å—æä¾›æœ¬åœ°AIæ¨¡åž‹çš„è°ƒç”¨æŽ¥å£ï¼Œä¸»è¦é€šè¿‡Ollamaæ¡†æž¶è°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„AIæ¨¡åž‹ã€‚
æ”¯æŒå¤šç§AIæ¨¡åž‹ï¼Œå½“å‰ä¸»è¦ä½¿ç”¨Qwen3:1.7bæ¨¡åž‹è¿›è¡Œæ–‡æœ¬å¤„ç†å’Œåˆ†æžã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è°ƒç”¨æœ¬åœ°AIæ¨¡åž‹è¿›è¡Œæ–‡æœ¬å¤„ç†
2. å¤„ç†å›¾åƒè¯†åˆ«ç»“æžœå¹¶ç”Ÿæˆæ™ºèƒ½åˆ†æž
3. æä¾›å¥åº·æ£€æŸ¥å’Œæ¨¡åž‹æµ‹è¯•åŠŸèƒ½
4. æ”¯æŒå¤‡ç”¨å¤„ç†æ–¹æ¡ˆ

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- åŸºäºŽOllama APIè¿›è¡Œæ¨¡åž‹è°ƒç”¨
- å¼‚æ­¥å¤„ç†æé«˜æ€§èƒ½
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œè¶…æ—¶æŽ§åˆ¶
- æ”¯æŒè‡ªå®šä¹‰æç¤ºè¯å’Œå‚æ•°
- æä¾›è¯¦ç»†çš„å¤„ç†åˆ†æž

æ”¯æŒçš„æ¨¡åž‹ï¼š
- Qwen3:1.7bï¼ˆé»˜è®¤ï¼‰
- å…¶ä»–Ollamaæ”¯æŒçš„æ¨¡åž‹

ä½œè€…ï¼šMCPå›¾åƒè¯†åˆ«ç³»ç»Ÿ
ç‰ˆæœ¬ï¼š1.0.0
"""

import asyncio                                    # å¼‚æ­¥ç¼–ç¨‹æ”¯æŒ
import json                                       # JSONæ•°æ®å¤„ç†
import httpx                                      # å¼‚æ­¥HTTPå®¢æˆ·ç«¯
from typing import Dict, Any, Optional           # ç±»åž‹æ³¨è§£
from api_server.config.settings import settings  # é…ç½®è®¾ç½®
from api_server.models.models import AIProcessResult  # æ•°æ®æ¨¡åž‹

class LocalAIProvider:
    """
    æœ¬åœ°AIæ¨¡åž‹æä¾›è€…

    è¿™ä¸ªç±»å°è£…äº†ä¸Žæœ¬åœ°AIæ¨¡åž‹çš„äº¤äº’é€»è¾‘ï¼Œé€šè¿‡Ollama APIè°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„AIæ¨¡åž‹ã€‚
    ä¸»è¦ç”¨äºŽå¤„ç†å›¾åƒè¯†åˆ«ç»“æžœï¼Œç”Ÿæˆæ™ºèƒ½åŒ–çš„æ–‡æœ¬åˆ†æžå’Œæ ¼å¼åŒ–è¾“å‡ºã€‚

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - AIæ¨¡åž‹è°ƒç”¨ï¼šé€šè¿‡HTTP APIè°ƒç”¨æœ¬åœ°AIæ¨¡åž‹
    - ç»“æžœå¤„ç†ï¼šæ™ºèƒ½å¤„ç†å›¾åƒè¯†åˆ«ç»“æžœ
    - æ–‡æœ¬ç”Ÿæˆï¼šç”Ÿæˆæ ¼å¼åŒ–çš„å¤„ç†ç»“æžœ
    - å¥åº·ç›‘æŽ§ï¼šæä¾›æ¨¡åž‹å¥åº·æ£€æŸ¥åŠŸèƒ½
    """

    def __init__(self):
        """
        åˆå§‹åŒ–æœ¬åœ°AIæä¾›è€…

        ä»Žé…ç½®ä¸­è¯»å–AIæ¨¡åž‹ç›¸å…³è®¾ç½®ï¼ŒåŒ…æ‹¬APIåœ°å€ã€æ¨¡åž‹åç§°å’Œè¶…æ—¶æ—¶é—´ã€‚
        """
        # ==================== åŸºç¡€é…ç½® ====================
        self.base_url = settings.LOCAL_AI_BASE_URL    # Ollama APIåŸºç¡€URL
        self.model = settings.LOCAL_AI_MODEL          # ä½¿ç”¨çš„AIæ¨¡åž‹åç§°
        self.timeout = 60                             # HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        print(f"ðŸ¤– æœ¬åœ°AIæä¾›è€…åˆå§‹åŒ–å®Œæˆ")
        print(f"ðŸ“¡ APIåœ°å€: {self.base_url}")
        print(f"ðŸ§  æ¨¡åž‹åç§°: {self.model}")
        print(f"â° è¶…æ—¶è®¾ç½®: {self.timeout}ç§’")
        
    async def _call_ollama(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        è°ƒç”¨Ollama APIè¿›è¡ŒAIæ¨¡åž‹æŽ¨ç†

        è¿™æ˜¯ä¸Žæœ¬åœ°AIæ¨¡åž‹äº¤äº’çš„æ ¸å¿ƒæ–¹æ³•ï¼Œé€šè¿‡HTTP APIè°ƒç”¨OllamaæœåŠ¡ã€‚
        æ”¯æŒè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯ï¼Œå¯ä»¥æŽ§åˆ¶æ¨¡åž‹çš„è¡Œä¸ºå’Œè¾“å‡ºæ ¼å¼ã€‚

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯ï¼Œæè¿°å…·ä½“çš„ä»»åŠ¡å’Œè¦æ±‚
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºè§„èŒƒ

        Returns:
            str: AIæ¨¡åž‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬

        Raises:
            Exception: å½“APIè°ƒç”¨å¤±è´¥ã€è¶…æ—¶æˆ–ç½‘ç»œé”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ==================== æž„é€ APIè¯·æ±‚ ====================
        url = f"{self.base_url}/api/generate"

        # æž„é€ è¯·æ±‚è´Ÿè½½ï¼ŒåŒ…å«æ¨¡åž‹å‚æ•°å’Œç”Ÿæˆé€‰é¡¹
        payload = {
            "model": self.model,                          # æŒ‡å®šä½¿ç”¨çš„AIæ¨¡åž‹
            "prompt": prompt,                             # ç”¨æˆ·æç¤ºè¯
            "stream": False,                              # ä¸ä½¿ç”¨æµå¼è¾“å‡º
            "options": {                                  # ç”Ÿæˆå‚æ•°
                "temperature": 0.7,                       # æŽ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0-1ï¼‰
                "top_p": 0.9,                            # æ ¸é‡‡æ ·å‚æ•°
                "max_tokens": 1000                        # æœ€å¤§ç”Ÿæˆtokenæ•°
            }
        }

        # å¦‚æžœæä¾›äº†ç³»ç»Ÿæç¤ºè¯ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if system_prompt:
            payload["system"] = system_prompt

        # ==================== æ‰§è¡ŒHTTPè¯·æ±‚ ====================
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                # å‘é€POSTè¯·æ±‚åˆ°Ollama API
                response = await client.post(url, json=payload)
                response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 

                # è§£æžJSONå“åº”å¹¶æå–ç”Ÿæˆçš„æ–‡æœ¬
                result = response.json()
                ai_response = result.get("response", "")

                print(f"âœ… AIæ¨¡åž‹è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(ai_response)} å­—ç¬¦")
                return ai_response

            except httpx.TimeoutException:
                # è¯·æ±‚è¶…æ—¶å¼‚å¸¸
                error_msg = f"AIæ¨¡åž‹è°ƒç”¨è¶…æ—¶ (>{self.timeout}ç§’)"
                print(f"â° {error_msg}")
                raise Exception(error_msg)
            except httpx.HTTPStatusError as e:
                # HTTPçŠ¶æ€ç é”™è¯¯
                error_msg = f"AIæ¨¡åž‹è°ƒç”¨å¤±è´¥: HTTP {e.response.status_code}"
                print(f"ðŸŒ {error_msg}")
                raise Exception(error_msg)
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸
                error_msg = f"AIæ¨¡åž‹è°ƒç”¨é”™è¯¯: {str(e)}"
                print(f"âŒ {error_msg}")
                raise Exception(error_msg)
    
    async def process_vision_result(self, vision_result: Dict[str, Any], original_text: str = "") -> AIProcessResult:
        """
        å¤„ç†å›¾ç‰‡è¯†åˆ«ç»“æžœ
        
        Args:
            vision_result: å›¾ç‰‡è¯†åˆ«ç»“æžœ
            original_text: åŽŸå§‹æ–‡æœ¬
            
        Returns:
            AIProcessResult: AIå¤„ç†ç»“æžœ
        """
        start_time = asyncio.get_event_loop().time()
        
        try:
            # æž„é€ ç³»ç»Ÿæç¤º
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®å¤„ç†åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æžå›¾ç‰‡è¯†åˆ«çš„ç»“æžœ
2. æå–å…³é”®ä¿¡æ¯
3. æ ¼å¼åŒ–è¾“å‡ºï¼Œæ·»åŠ é€‚å½“çš„æ ‡è¯†
4. ç¡®ä¿è¾“å‡ºç®€æ´æ˜Žäº†

è¯·ç”¨ä¸­æ–‡å›žå¤ï¼Œä¿æŒä¸“ä¸šå’Œå‡†ç¡®ã€‚"""
            
            # æž„é€ ç”¨æˆ·æç¤º
            user_prompt = f"""
è¯·åˆ†æžä»¥ä¸‹å›¾ç‰‡è¯†åˆ«ç»“æžœï¼Œå¹¶è¿›è¡Œå¤„ç†ï¼š

åŽŸå§‹æ–‡æœ¬: {original_text}

å›¾ç‰‡è¯†åˆ«ç»“æžœ:
- è¯†åˆ«ç±»åž‹: {vision_result.get('type', 'æœªçŸ¥')}
- è¯†åˆ«å†…å®¹: {vision_result.get('content', 'æ— å†…å®¹')}
- ç½®ä¿¡åº¦: {vision_result.get('confidence', 0)}

è¯·æ ¹æ®è¯†åˆ«ç»“æžœç”Ÿæˆä¸€ä¸ªå¤„ç†åŽçš„æ–‡æœ¬ï¼Œæ ¼å¼ä¸º: [AIè¯†åˆ«] + å…³é”®ä¿¡æ¯æ‘˜è¦

è¦æ±‚:
1. æå–æœ€é‡è¦çš„ä¿¡æ¯
2. ä¿æŒç®€æ´æ˜Žäº†
3. æ·»åŠ [AIè¯†åˆ«]æ ‡è¯†
4. å¦‚æžœæ˜¯èº«ä»½è¯ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œè¯·é€‚å½“è„±æ•
"""
            
            # è°ƒç”¨AIæ¨¡åž‹
            ai_response = await self._call_ollama(user_prompt, system_prompt)
            
            # å¤„ç†AIå“åº”
            processed_text = self._format_ai_response(ai_response, vision_result)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = asyncio.get_event_loop().time() - start_time
            
            # ç”ŸæˆAIåˆ†æž
            ai_analysis = self._generate_analysis(vision_result, ai_response)
            
            return AIProcessResult(
                original_text=original_text,
                processed_text=processed_text,
                ai_analysis=ai_analysis,
                confidence=min(0.95, vision_result.get('confidence', 0.8) * 0.9),  # ç¨å¾®é™ä½Žç½®ä¿¡åº¦
                processing_time=processing_time
            )
            
        except Exception as e:
            # å¦‚æžœAIå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            return self._fallback_processing(vision_result, original_text, str(e))
    
    def _format_ai_response(self, ai_response: str, vision_result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–AIå“åº”"""
        
        # æ¸…ç†AIå“åº”
        cleaned_response = ai_response.strip()
        
        # å¦‚æžœAIå“åº”å·²ç»åŒ…å«[AIè¯†åˆ«]æ ‡è¯†ï¼Œç›´æŽ¥è¿”å›ž
        if "[AIè¯†åˆ«]" in cleaned_response:
            return cleaned_response
        
        # å¦åˆ™æ·»åŠ æ ‡è¯†
        if cleaned_response:
            return f"[AIè¯†åˆ«] {cleaned_response}"
        else:
            # å¦‚æžœAIå“åº”ä¸ºç©ºï¼Œä½¿ç”¨å¤‡ç”¨æ ¼å¼
            content = vision_result.get('content', 'æ— æ³•è¯†åˆ«å†…å®¹')
            return f"[AIè¯†åˆ«] {content[:100]}..."  # æˆªå–å‰100å­—ç¬¦
    
    def _generate_analysis(self, vision_result: Dict[str, Any], ai_response: str) -> str:
        """ç”ŸæˆAIåˆ†æžç»“æžœ"""
        
        vision_type = vision_result.get('type', 'æœªçŸ¥')
        confidence = vision_result.get('confidence', 0)
        
        analysis_parts = []
        
        # è¯†åˆ«ç±»åž‹åˆ†æž
        type_analysis = {
            'text_recognition': 'æ–‡å­—è¯†åˆ«',
            'object_detection': 'ç‰©ä½“æ£€æµ‹', 
            'document_analysis': 'æ–‡æ¡£åˆ†æž',
            'face_recognition': 'äººè„¸è¯†åˆ«',
            'mock': 'Mockæµ‹è¯•'
        }
        
        analysis_parts.append(f"è¯†åˆ«ç±»åž‹: {type_analysis.get(vision_type, vision_type)}")
        
        # ç½®ä¿¡åº¦åˆ†æž
        if confidence >= 0.9:
            confidence_desc = "é«˜ç½®ä¿¡åº¦"
        elif confidence >= 0.7:
            confidence_desc = "ä¸­ç­‰ç½®ä¿¡åº¦"
        else:
            confidence_desc = "ä½Žç½®ä¿¡åº¦"
        
        analysis_parts.append(f"è¯†åˆ«è´¨é‡: {confidence_desc} ({confidence:.2f})")
        
        # AIå¤„ç†çŠ¶æ€
        if ai_response and len(ai_response.strip()) > 10:
            analysis_parts.append("AIå¤„ç†: æˆåŠŸå®Œæˆæ™ºèƒ½åˆ†æžå’Œæ ¼å¼åŒ–")
        else:
            analysis_parts.append("AIå¤„ç†: ä½¿ç”¨å¤‡ç”¨å¤„ç†æ–¹æ¡ˆ")
        
        return " | ".join(analysis_parts)
    
    def _fallback_processing(self, vision_result: Dict[str, Any], original_text: str, error: str) -> AIProcessResult:
        """å¤‡ç”¨å¤„ç†æ–¹æ¡ˆ"""
        
        # ç®€å•çš„æ–‡æœ¬å¤„ç†
        content = vision_result.get('content', original_text)
        processed_text = f"[AIè¯†åˆ«] {content[:200]}..."  # æˆªå–å‰200å­—ç¬¦
        
        return AIProcessResult(
            original_text=original_text,
            processed_text=processed_text,
            ai_analysis=f"AIå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {error}",
            confidence=0.5,  # é™ä½Žç½®ä¿¡åº¦
            processing_time=0.1
        )
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•ç®€å•çš„AIè°ƒç”¨
            test_prompt = "è¯·å›žå¤'å¥åº·æ£€æŸ¥é€šè¿‡'"
            response = await self._call_ollama(test_prompt)
            
            return {
                "status": "healthy",
                "model": self.model,
                "base_url": self.base_url,
                "response_preview": response[:50] + "..." if len(response) > 50 else response,
                "version": "1.0.0"
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "model": self.model,
                "base_url": self.base_url,
                "error": str(e),
                "version": "1.0.0"
            }
    
    async def test_model(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡åž‹åŠŸèƒ½"""
        try:
            test_cases = [
                {
                    "name": "åŸºç¡€å¯¹è¯æµ‹è¯•",
                    "prompt": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                    "expected_keywords": ["åŠ©æ‰‹", "å¸®åŠ©", "AI"]
                },
                {
                    "name": "æ•°æ®å¤„ç†æµ‹è¯•", 
                    "prompt": "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬æ·»åŠ [AIè¯†åˆ«]æ ‡è¯†ï¼šèº«ä»½è¯å·ç 123456789",
                    "expected_keywords": ["AIè¯†åˆ«", "èº«ä»½è¯"]
                }
            ]
            
            results = []
            
            for test_case in test_cases:
                try:
                    response = await self._call_ollama(test_case["prompt"])
                    
                    # æ£€æŸ¥å…³é”®è¯
                    keywords_found = [kw for kw in test_case["expected_keywords"] if kw in response]
                    
                    results.append({
                        "name": test_case["name"],
                        "success": len(keywords_found) > 0,
                        "response_length": len(response),
                        "keywords_found": keywords_found,
                        "response_preview": response[:100] + "..." if len(response) > 100 else response
                    })
                    
                except Exception as e:
                    results.append({
                        "name": test_case["name"],
                        "success": False,
                        "error": str(e)
                    })
            
            success_count = sum(1 for r in results if r.get("success", False))
            
            return {
                "overall_success": success_count == len(test_cases),
                "success_rate": f"{success_count}/{len(test_cases)}",
                "model": self.model,
                "test_results": results
            }
            
        except Exception as e:
            return {
                "overall_success": False,
                "error": str(e),
                "model": self.model
            }

# åˆ›å»ºå…¨å±€å®žä¾‹
local_ai_provider = LocalAIProvider()
